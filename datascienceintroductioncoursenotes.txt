
I define data scientist as someone Who finds solutions to problems by analyzing big or small data using appropriate tools and then tells stories to communicate her findings to the relevant stakeholders. I do not use the data size as a restrictive clause. A data below a certain arbitrary threshold does not make one less Of a data scientist. Nor is my definition Of a data scientist restricted to particular analytic tools, such as machine learning. As long as one has a curious mind, fluency in analytics, and the ability to communicate the findings, I consider the person a data scientist. 

-analyze data using appropriate tools
-explain the findings using story telling
-curious mind, skills in analytics & ability to tell stories = data scientist
-thinking compunationally
-everyone has their own opinion what data science is

Dr. Schutt defined a data scientist as someone Who is part computer scientist, part software engineer, and part statistician (Miller, 2013). But that's the definition Of an average data scientist. The best, she contended, tend to be really curious people, thinkers who ask good questions and are O.K. dealing with unstructured situations and trying to find structure in them. 

-curiousity is one of the key elements in being a good data scientist

It is important to realize that one who tries to set arbitrary thresholds to exclude others is likely to run into inconsistencies. The goal should be to define data science in a more exclusive, discipline- and platform- size-free context where data-centric problem solving and the ability to weave strong narratives take center Stage. 

-you should not limit your views when defining data science/scientists

Dr. patil told the Guardian newspapRr in 2012 that a "data scientist is that unique blend of skills that can both unlock the insights of data and tell a fantastic story via the data.' What is admirable about Dr. Patil's definition is that it is inclusive of individuals Of various academic backgrounds and training, and does not restrict the definition Of a data scientist to a particular tool or subject it to a certain arbitrary minimum threshold Of data size. 

-not only it is important to be able to read&analyze data, but you should be able to make interesting story out of it

    The typical work day for a Data Scientist varies depending on what type of project they are working on.
    Many algorithms are used to bring out insights from data. 
    Accessing algorithms, tools, and data through the Cloud enables Data Scientists to stay up-to-date and collaborate easily.

-
V's of big data; Velocity, Volume, Variety Veracity and Value

Velocity=speed at which data accumulates
Volume=scale of the data
Variety= diversity of the data. structured (fits in rows and columns)  and unstructured data(gathered from around the web eg videos, pictures, blog posts etc.)
Veracity=quality and origin of data and its conformity to facts and accuracy. is the data real or false?
value= ability and need to turn data into value. can have different meaning than profit, eg customer satisfaction. value is the main reasons people invest time to big data, to gain something good(value) from it.


Apache Hadoop is an open source framework that is used to efficiently store and process large datasets ranging in size from gigabytes to petabytes of data. Instead of using one large computer to store and process the data, Hadoop allows clustering multiple computers to analyze massive datasets in parallel more quickly.

According to Dr. White, his students, who are mostly aspiring data scientists, need to learn many tools such as Python, UNIX commands, pandas, and Jupyter notebook.

My definition of big data is data that is large enough
and has enough volume and velocity
that you cannot handle it with traditional database systems.


Some of our statisticians think big data
is something you can't fit on a thumb drive.


Big data, to me, was started by Google.
When Google tried to figure out how they were,
when Larry Page and Sergey Brin wanted to, basically,
figure out how to solve their page rank algorithm,
there was nothing out there.

They were trying to store all of the web pages in the world,
and there was no technology, there was no way to do this,
and so they went out and developed this approach,
which has now become, Hadoop has copied it,
but this is where all these large,
big data clusters are found. 

datamining

-set up goals
-identify key questions
-costs and benefits, cost benefit trade-off. more accuracy means more cost. data affects the cost too; how its gathered, quality of it, is it readily available or not.
-determine in advance the expected accuracy & usefulness of the results
-preprocessing; going through raw/unstructured data (usually contains errors,irrelevancies). errors must be flagged. have method of dealing with missing data and determining is it randomly or systematically missing. randomly missing data is easier to handle with than systematically missing data. syst. missing data has an impact on the results and must be determined to avoid systematic biases. eg some group of people dont want to tell their incomes.
-find appropriate format in which data must be stored. PCA (Principal Component Analysis) can reduce number of attributes without too much loss in information. variables may need to be transformed to help explain the phenomenon being studied. eg aggregating income from all sources will develop representative indicator for the invidual income. often transformation of variables from one type to another is needed. eg continuous variable for income to categorial variable with low,medium and high-income inviduals.
-storing data in a format that makes it favorable for data mining. format must be unrestricted and give read/write privileges to the data scientist. store data in servers or media that is secure and in one place rather than scattered on different places. data safety and privacy = no1 concern for storing data.
-mining data includes data analysis methods, non- and parametric methods, machine learning algorithms. good place to start is visualization of data. multidimensional views of thedata using dataminingsoftware are helpful in understanding the hidden trends in the data set and creating a first hand view of the results.
-formal evaluation of the results. could incl testing predictive capabilities of the models on observed data to see how effective and efficient the algorithms have been in reproducing data, in-sample forecast. results are shared with key stakeholders for feedback, which is incorporated in later iterations od data mining to improve the process.

-Deep Learning is a type of Machine Learning that simulates human decision-making using neural networks.
-Machine Learning has many applications, from recommender systems that provide relevant choices for customers on commercial websites, to detailed analysis of financial markets.

data science in business

-Using Data Science techniques to understand and analyze the large data sets available
today has a huge impact on human lives.
It can provide targeted information to help healthcare professionals give the best treatment
to patients, or help predict natural disasters so that people can prepare early

how company should get started in data science?
-start collecting data. archive it, dont overwrite. data never gets old.
-proper documentation, understandable format.
-measuring things. its impossible to improve without measuring.
-team of data scientists.

applications of data science
-recommendation engines. using algorithms based on customer behaviour and history.

final deliverable
-goal of analytics is to communicate findings to the concerned who use these findings to create policy or strategy
-analytics summarie findings in tables and plots. using these plots data scientist should build a narrative to communicate the findings. final deliverable is essay or report in academia. 1000-7000words in length.
-in consulting and business, it can be a small document 1500 or fewer words, illustrated with tables and plots. or more comprehensive report with hundreds of pages.
-

ten components in report
-structure of the report; brief or longer? depends on the purpose
-cover page, table of contents, executive summary/abstract, introductory, literature review, methodology section, results, detailed contents, discussion, conclusion, acknowledgements, references and appendices (if needed)

1.summary/abstract
2.Introductory
3.Literature review
4.Methodology section
5.Results
6. Detailed contents
7. Discussion
8. Conclusion
9. References.
10.Acknowledgements

